---
title: "Text mining"
subtitle: "na przykładzie Google News"
author: "&copy; Łukasz Wawrowski"
date: ""
output:
  xaringan::moon_reader:
    css: ["default.css", "default-fonts.css"]
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r include=FALSE}
library(tidyverse)
library(stringi)
library(stringdist)
library(countdown)
```

# Plan szkolenia

1. Wprowadzenie

2. Podobieństwo tekstów

3. Metoda n-gramów

4. Lematyzacja

---

# Łukasz Wawrowski

- Uniwersytet Ekonomiczny w Poznaniu

  - statystyka
  
  - ubóstwo
  
--

- DOJI S.A.

  - projekt GAMEINN
  
  - sztuczna inteligencja
  
--

- Bloger @ [wawrowski.edu.pl](http://www.wawrowski.edu.pl/)

---

# Why R?

1. Natywna obsługa danych

2. Łatwe w obsłudze repozytorium pakietów

3. [Programowanie funkcyjne](https://adv-r.hadley.nz/fp.html)

4. [Metaprogramowanie](https://adv-r.hadley.nz/metaprogramming.html)

???

https://blog.shotwell.ca/posts/why_i_use_r/

---

# Źródła danych

- [Mining the Social Web](https://www.webpages.uidaho.edu/~stevel/504/Mining-the-Social-Web-2nd-Edition.pdf)

- [Text Mining with R](https://www.tidytextmining.com/)

--

- prezentacja ze szkolenia: [lwawrowski.github.io/tm/](http://lwawrowski.github.io/tm/)

---

# Wprowadzenie

Text mining - ogólna nazwa metod eksploracji danych służących do wydobywania danych z tekstu i ich późniejszej obróbki [[Wikipedia](https://pl.wikipedia.org/wiki/Text_mining)].

Zastosowania:

- łączenie niedeterministyczne

- analiza wydzwięku/sentymentu

- przetwarzanie języka naturalnego

- identyfikacja duplikatów

---

# Tekst - nośnik wiedzy

- dane wejściowe (imona, nazwiska, kategorie)

- dane wyjściowe (skrypty, PDF, strony internetowe)

---

# Zestaw analityka - tidyverse

.pull-left[
```
install.packages("tidyverse")
```

- readxl - wczytywanie plików Excela
- haven - wczytywanie plików SAS, SPSS, Stata
- jsonlite - wczytywanie JSON
- xml2 - wczytywanie XML
- httr - dostęp do stron html
- rvest - web scraping
- DBI - dostęp do baz danych
- lubridate - operacje na datach
- hms - operacje na czasie dnia
- blob - przechowywanie danych binarnych
- magrittr - przetwarzanie potokowe `%>%`
- glue - łączenie tekstów

]

.pull-right[
```
library("tidyverse")
```

- ggplot2 - wykresy
- dplyr - przetwarzanie danych
- tidyr - porządkowoanie danych
- readr - wczytywanie plików tekstowych
- purrr - programowanie funkcyjne
- tibble - format dla zbiorów
- stringr - operacje na tekstach
- forcats - operacje na czynnikach

]

[Strona projektu Tidyverse](https://www.tidyverse.org/)

---

# Zbiór googlenews

Dane pobierane przez [newsapi.org](https://newsapi.org/) w dwóch okresach:

- [19.08.2019 - 1.09.2019](data/googlenews.RData)

- 3.01.2019 - 8.01.2020

Zawartość zbiorów:

- id
- name - nazwa źródła
- author - nazwa autora
- title - nagłówek artykułu
- description - dłuższy opis artykułu
- url - adres oryginalnego artykułu
- urlToImage - adres miniaturki
- publishedAt - data publikacji
- content - dłuższa część artykułu
- category - jedna z 7 kategorii artykułu: business, entertainment, general, health, science, sports, technology

---

class: inverse

# Zadanie

Przeprowadź eksploracyjną analizę danych dla zbioru googlenews. 

1. Kto najczęściej publikował?

2. Kiedy pojawiały się artykuły?

3. Ile jest artykułów w danej kategorii?

4. W jaki dzień tygodnia pojawia się najwięcej artykułów?

---

# Odległość tekstów

Miara podobieństwa pomiędzy dwoma tekstami. Własności:

1. nieujemność $d(s,t) \geq 0$

2. identyczność $d(s,t) = 0 \Longleftrightarrow s = t$

3. symetria $d(s,t)=d(t,s)$

4. nierówność trójkąta $d(s,u) \leq d(s,t) + d(t,u)$

---

# Pakiet stringdist

[stringdist](https://cran.r-project.org/web/packages/stringdist/index.html) - Approximate String Matching and String Distance Functions.

[van der Loo M (2014). “The stringdist package for approximate string matching.” _The R Journal_, *6*, 111-122.](https://journal.r-project.org/archive/2014-1/loo.pdf)

Funkcje

```{r}
stringdist(a = "tekst1", b = "tekst2", method = "osa", q = 1, p = 0)
```

```{r}
stringdistmatrix(a = c("tekst1", "tekst2"), 
                 b = "tekst2", 
                 method = "osa", 
                 q = 1, 
                 p = 0)
```

---

# Odległość Hamminga

Liczba różnych znaków na tej samej pozycji w obu ciągach. Może być obliczona tylko dla tekstów o tej samej długości.

```{r}
stringdist("wydawnictwo", "wydawnicwto", method = "hamming")
```

```{r}
stringdist("przynajmniej", "bynajmniej", method = "hamming")
```

---

# Najdłuższy wspólny podłańcuch 

Znajduje najdłuższy ciąg znaków (ang. longest common substring) leżących obok siebie w obu ciągach, natomiast odległość to liczba niesparowanych znaków.

```{r}
stringdist("wydawnictwo", "wydawnicwto", method = "lcs")
```

```{r}
stringdist("przynajmniej", "bynajmniej", method = "lcs")
```

---

# Odległość Levenshteina

Odległość to liczba kroków wymaganych do przekształcenia jednego ciągu znaków w drugi. Działania jakie można wykonywać to wstawienie nowego znaku, usunięcie znaku lub zamianę znaku na nowy znak.

```{r}
stringdist("wydawnictwo", "wydawnicwto", method = "lv")
```

```{r}
stringdist("przynajmniej", "bynajmniej", method = "lv")
```

---

# Odległość Damerau-Levenshteina 

Podobnie jak odległość Levenshteina, ale wprowadza działanie polegające na zamianie miejscami sąsiadujących znaków.

```{r}
stringdist("wydawnictwo", "wydawnicwto", method = "dl")
```

```{r}
stringdist("przynajmniej", "bynajmniej", method = "dl")
```

---

### Odległość Damerau-Levenshteina z ograniczeniem 

_Optimal string aligment_ - podobnie jak odległość Damerau-Levenshteina, ale każdy podłańcuch może być modyfikowany tylko raz. Nie spełnia nierówności trójkąta.

```{r}
stringdist("wydawnictwo", "wydawnicwto", method = "osa")
```

```{r}
stringdist("przynajmniej", "bynajmniej", method = "osa")
```

---

# Q-gram

Ciąg zawierający $q$ następujących po sobie znaków ( $v(s;q)$ ). Odległość można zdefiniować jako liczbę unikalnych $q$ gramów.

```{r}
qgrams("przynajmniej", "bynajmniej", q=2)
```

```{r}
stringdist("przynajmniej", "bynajmniej", method = "qgram", q = 2)
```

---

# Q-gram

```{r}
qgrams("wydawnictwo", "wydawnicwto", q=2)
```

```{r}
stringdist("wydawnictwo", "wydawnicwto", method = "qgram", q = 2)
```

---

# Odległość Jaccarda

Jeśli $Q(s;q)$ oznacza zbiór unikalnych $q$ gramów w ciągu $s$ to odległość Jaccarda jest zdefiniowana jako:

$$d(s,t;q)=1-\frac{|Q(s;q) \cap Q(t;q)|}{|Q(s;q) \cup Q(t;q)|}$$

gdzie pionowe kreski oznaczają liczbę unikalnych elementów w zbiorze. Odległość Jaccarda zawiera się w przedziale od 0 do 1, gdzie 0 oznacza teksty identyczne ( $Q(s;q) = Q(t;q)$ ), a 1 zupełnie różne ( $Q(s;q) \cap Q(t;q)=\emptyset$ )

```{r}
stringdist("przynajmniej", "bynajmniej", method = "jaccard", q = 2)
```


```{r}
stringdist("wydawnictwo", "wydawnicwto", method = "jaccard", q = 2)
```

---

# Odległość cosinusowa

Odległość cosinusowa jest zdefiniowana jako:

$$d(s,t;q)=1-\frac{v(s;q)\cdot v(t;q)}{||v(s;q)||_2||v(t;q)||_2}$$

Odległość cosinusowa wynosi 0 jeśli oba ciągi są takie same oraz 1, gdy nie ma żadnych wspólnych $q$ gramów.

```{r}
stringdist("przynajmniej", "bynajmniej", method = "cosine", q = 2)
```


```{r}
stringdist("wydawnictwo", "wydawnicwto", method = "cosine", q = 2)
```

---

# Odległość Jaro

Średnia z odsetka wspólnych znaków w pierwszym ciągu, odsetka wspólnych znaków w drugim ciągu oraz odsetka wspólnych znaków nie wymagających transpozycji. Przyjmuje wartości od 0 do 1, gdzie 0 to teksty identyczne, a 1 zupełnie różne.

```{r}
stringdist("wydawnictwo", "wydawnicwto", method = "jw")
```

```{r}
stringdist("przynajmniej", "bynajmniej", method = "jw")
```

---

# Odległość Jaro-Winklera 

Zwiększenie podobieństwa ciągów, które według odległości Jaro są do siebie podobne. Kara za wykonanie modyfikacji z przedziału $0-0.25$.

```{r}
stringdist("wydawnictwo", "wydawnicwto", method = "jw", p = 0.1)
```

```{r}
stringdist("przynajmniej", "bynajmniej", method = "jw", p = 0.1)
```

???

[źródło](https://www.joyofdata.de/blog/comparison-of-string-distance-algorithms/)

---

# Soundex

Ciąg znaków jest tłumaczony do zapisu fonetycznego. Jeżeli zapis fonetyczny jest taki sam to odległość wynosi 0, a w przeciwnym przypadku 1. Działa wyłącznie dla liter a-z.

```{r}
phonetic("beer")
```


```{r}
stringdist("beer", "wine", method = "soundex")
```


```{r}
stringdist("bear", "beer", method = "soundex")
```

---

# Podsumowanie

Im mniej tym większe podobieństwo

- najdłuższy wspólny podłańcuch (`lcs`)

- odległość Levenshteina (`lv`)

- odległość Damerau-Levenshteina (`dl`)

- odległość Damerau-Levenshteina z ograniczeniem (`osa`)

- q-gram (`qgram`)

Z przedziału 0-1 - 0 oznacza identyczne ciągi

- odległość Jaccarda (`jaccard`)

- odległość cosinusowa (`cosine`)

- odległość Jaro i Jaro-Winklera (`jw`)

- soundex

---

# Wykres Sankeya

Pakiet _ggalluvial_

```{r eval=FALSE}
ggplot(data, aes(y = value, axis1 = from, axis2 = to)) +
  geom_alluvium(aes(fill = value)) +
  geom_stratum(width = 1/12, fill = "black", color = "grey") +
  geom_label(stat = "stratum", infer.label = TRUE)
```

---

# Wykres strunowy (chord diagram)

Pakiet _circlize_

```{r eval=FALSE}
chordDiagram(data)
```

Zbiór danych wejściowych powiniem zawierać trzy kolumny o następujących nazwach:

- `from`

- `to`

- `value`

Wówczas wykres stworzy się automatycznie.

---

# Wizualizacja na grafie

Potrzebne pakiety _igraph_ oraz _ggraph_.

```{r eval=FALSE}
wordnetwork <- graph_from_data_frame(data)

ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "lightblue") +
  geom_node_text(aes(label = name), col = "black", size = 4) +
  theme_graph()
```

---

# Przykład

Podczas kolokwium studenci udostępniali swoje kody R, które wykorzystali do rozwiązania zadań. Za pomocą analizy podobieństwa sprawdź czy można na tej podstawie zidentyfikować studentów, którzy nie pracowali samodzielnie.

Zbiór danych [kody](data/kody.RData):

- id - identyfikator osoby i grupy

- odp - kod R 

---

class: inverse

# Zadanie

Nazwiska lubią być przekręcane. Na podstawie [bazy nazwisk w województwie śląskim](data/nazwiska.RData) zidentyfikuj te najbardziej podobne z wykorzystaniem różnych miar odległości. Ze względu na bardzo dużą liczbę obserwacji warto ograniczyć się do nazwisk występujących np. conajmniej 200 razy. 

Jak można przekręcić nazwisko "Kowalski"? Jakie nazwiska mogą powstać poprzez zmianę tylko jednej litery w tym nazwisku?

---

# Przetwarzanie tekstu w R

Operacje na wektorach.

--

```{r}
tekst <- "Text mining w R"

nchar(tekst)

charToRaw(tekst)

rawToChar(rev(charToRaw(tekst)))
```

--

Bazowe funkcje w R są mało intuicyjne.

---

# Przetwarzanie napisów

Pakiet **stringi** autorstwa Marka Gągolewskiego zawierający 234 funkcje.

- prosty w użyciu - funkcje zaczynają się przedrostkiem `stri_`

- wiele przydatnych funkcji

- bardzo szybki

- bardzo _eRowy_

```{r}
paste(letters[1:3], NA)
stri_join(letters[1:3], NA)
```

Pakiet **stringr** zawiera 42 funkcje z pakietu **stringi**.

???

https://drive.google.com/drive/u/0/folders/0B-ZJyVlQBsqlcXRsaHI5bDdtaUk

---

# Metoda n-gramów

Identyfikacja pojawiających się najczęściej pojedyńczych słów, a także par oraz trójek **sąsiadujących** ze sobą słów. 

1. Oczyszczenie danych

2. Usunięcie stop-słów

   - dla wybranych języków - funkcja `stopwords` z pakietu _tm_
   
   - dla języka polskiego - [lista 350 słów](https://github.com/bieli/stopwords/blob/master/polish.stopwords.txt)
   
3. Znalezienie wszystkich n-gramów

4. Prezentacja wyników jako wykresu słupkowego lub chmury słów

---

# Czyszczenie danych

- usunięcie znaków niealfanumerycznych `str_replace_all("[^[:alnum:] ]", " ")`

- usunięcie zbędnych spacji `str_squish()`

- usunięcie [stop-słów](data/polish_stopwords.txt) `read_lines("data/polish_stopwords.txt")`

- usunięcie cyfr i liczb `is.na(as.numeric())`

- usunięcie krótkich ciągłów `str_length()`

---

# Szukanie n-gramów

Funkcja `unnest_tokens` z pakietu _tidytext_ służy do zamiany długiego tekstu na n-gramy.

```{r eval=FALSE}
unnest_tokens(tbl = zbior, output = nazwa_kolumny, input = nazwa_kolumny,
              token = "words", n = 1, drop = FALSE)
```

- `tbl` - nazwa analizowanego zbioru

- `output` - nazwa kolumny zawierającej n-gramy

- `input` - nazwa kolumny zawierającej analizowany tekst

- `token` - domyślna wartość to "words" oznaczająca analizę pojedyńczych słów, wartość "ngrams" pozwala analizować n-gramy

- `n` - liczba słów w n-gramie

- `drop` - czy usuwać kolumnę wejściową (domyślnie `TRUE`)

W rezultacie otrzymuje się zbiór zawierający jeden n-gram w wierszu. 

Analizowana kolumna nie może zawierać braków danych `NA`.

---

class: inverse

# Zadanie

Na podstawie artykułów dotyczących sportu przeprowadź analizę n-gramów. Kto był najczęściej przywoływanym sportowcem w badanym okresie? 

---

# Lematyzacja i tagowanie

Lematyzacja - sprowadzanie formy fleksyjnej wyrazu do postaci słownikowej.

Tagowanie - przyporządkowanie wyrazu do części mowy.

---

# UDPipe

Biblioteka stworzona przez Instytut Lingwistyki Formalnej i Stosowanej na Uniwersytecie Karola w Pradze. 

Stworzony niezależnie pakiet R [udpipe](https://bnosac.github.io/udpipe/en/index.html).

1. Pobranie modelu dla języka polskiego `mod <- udpipe_download_model(language = "polish-lfg")`

2. Załadowanie tego modelu `model <- udpipe_load_model("sciezka")`

3. Tagowanie i lematyzacja tekstu `udpipe_annotate()`

---

# Zbiór wynikowy

W zbiorze wynikowym znajdują się następujące kolumny:

- doc_id - identyfikator tekstu
- paragraph_id - identyfikator paragrafu
- sentence_id - identyfikator zdania
- sentence - analizowane zdanie
- token_id - identyfikator części zdania
- token - wyodrębniona część zdania
- lemma - postać słownikowa tokenu
- upos - część mowy
- xpos - tag części mowy
- feats - lista cech morfologicznych (przypadek, rodzaj)
- head_token_id - identyfikator podmiotu
- dep_rel - relacje zależności
- deps
- misc - dodatkowe przypisy

---

# Części mowy

- rzeczownik (NOUN)
- czasownik (VERB)
- przymiotnik (ADJ - adjective)
- przysłówek (ADV - adverb)
- zaimek (PRON - pronoun)
- nazwy własne (PROPN - proper noun)
- przyimki określajace położenie (ADP - adposition)
- wykrzyknik (INTJ - interjection)
- spójnik koordynujący (CCONJ - coordinating conjunction)
- znak interpunkcyjny (PUNCT - punctuation)
- liczebnik (NUM - numeral)
- partykuła (PART - particle)
- czasownik posiłkowy (AUX - auxiliary)
- określnik (DET - determiner)
- spójnik podrzędny (SCONJ - subordinating conjunction)
- symbol (SYM)

[źródło](https://universaldependencies.org/u/pos/index.html)

---

# Kontekst

Dla słów ważny jest kontekst. 

- RAKE (Rapid Automatic Keyword Extraction)

- Punktowa wzajemna informacja (Pointwise mutual information)

- Parts of Speech phrase sequence detection

Wyszukiwanie słów kluczowych niekoniecznie występujących obok siebie.

---

# RAKE

1. Eliminacja słów nie mających wpływu na treść

2. Stworzenie macierzy współwystępowania

3. Obliczenie wyniku na podstawie macierzy

4. $T$ fraz z najwyższym wynikiem uznawane są za słowa kluczowe

???

https://monkeylearn.com/keyword-extraction/

---

# RAKE

Funkcja

```{r eval=FALSE}
keywords_rake(x, term, group, relevant, ngram_max = 2, n_min = 2)
```

- `x` - obiekt z funkcji `udpipe_annotate()`

- `term` - kolumna do analizy

- `group` - identyfikator dokumentu

- `relevant` - wybór wierszy uwzględnionych w analizie np. `x$upos %in% c("NOUN", "ADJ")`

- `ngram_max` - maksymalna liczba wyrazów w kluczowej frazie

- `n_min` - minimalna liczba wystąpienia wyrazu, żeby był brany pod uwagę w analizie

---

# PMI

Analiza kolokacji słów na podstawie częstości występowania.

$$PMI=log_2\frac{p(x,y)}{p(x)p(y)}$$

- $p(x,y)$ - częstość występowania pary słów $x$ i $y$

- $p(x)$ - częstość występowania słowa $x$

- $p(y)$ - częstość występowania słowa $y$

PMI równe 0 oznacza, że analizowane słowa są statystycznie niezależne. Dodania wartość PMI oznacza, że analizowane słowa występują częściej niż wynikałoby to z założenia o niezależności. 

???

https://en.wikipedia.org/wiki/Pointwise_mutual_information

---

# PMI

```{r eval=FALSE}
keywords_collocation(x, term, group, ngram_max = 2, n_min = 2)
```

- `x` - obiekt z funkcji `udpipe_annotate()`

- `term` - kolumna do analizy

- `group` - identyfikator dokumentu

- `ngram_max` - maksymalna liczba wyrazów w kluczowej frazie

- `n_min` - minimalna liczba wystąpienia wyrazu, żeby był brany pod uwagę w analizie

---

# Współwystępowanie

```{r eval=FALSE}
cooccurrence(x, term, group)
```

- `x` - obiekt z funkcji `udpipe_annotate()`

- `term` - kolumna do analizy

- `group` - identyfikator dokumentu

Funkcja zwróci bigramy wraz z liczbą wystąpień.

---

class: inverse

# Zadanie

Na podstawie danych z googlenews z okresu 3-8.01.2020 przeprowadź kompleksową analizę tekstu wykorzystując wszystkie poznane metody. 

---

class: center, middle, inverse

# Dziękuję za uwagę!
